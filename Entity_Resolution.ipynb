{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadGloveModel(gloveFile, size_emb):\n",
    "    print(\"Loading Glove Model...\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    print(\"...Done! \", len(model), \" words loaded!\")\n",
    "    if '<unk>' not in model and size_emb == 50:\n",
    "        model['<unk>'] = np.asarray([0.072617, -0.51393,   0.4728,   -0.52202,  -0.35534,   0.34629,   0.23211, 0.23096,   0.26694,   0.41028,   0.28031,   0.14107,  -0.30212,  -0.21095, -0.10875,  -0.33659,  -0.46313,  -0.40999,   0.32764,   0.47401,  -0.43449, 0.19959,  -0.55808,  -0.34077,   0.078477,  0.62823,   0.17161,  -0.34454, -0.2066,    0.1323,   -1.8076,   -0.38851,   0.37654,  -0.50422,  -0.012446, 0.046182,  0.70028,  -0.010573, -0.83629,  -0.24698,   0.6888,   -0.17986, -0.066569, -0.48044,  -0.55946,  -0.27594,   0.056072, -0.18907,  -0.59021, 0.55559], dtype='float32')\n",
    "    return model\n",
    "\n",
    "\n",
    "class NLP(nn.Module):\n",
    "    def __init__(self, word_embed, word_embed_size, n_attrs, device):\n",
    "        super(NLP, self).__init__()\n",
    "\n",
    "        self.n_attr = n_attrs\n",
    "        self.size_embed = word_embed_size\n",
    "        self.words = loadGloveModel(word_embed, word_embed_size)\n",
    "        self.fc1 = nn.Linear(n_attrs, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "        self.probs = nn.LogSoftmax(dim=-1)\n",
    "        self.device = device\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.normal_(self.fc1.weight, std=1)\n",
    "        nn.init.normal_(self.fc1.bias, std=0.01)\n",
    "        nn.init.normal_(self.fc2.weight, std=1)\n",
    "        nn.init.normal_(self.fc2.bias, std=0.01)\n",
    "\n",
    "    def create_embed(self, x1):\n",
    "        t1 = []\n",
    "\n",
    "        for i in range(self.n_attr):\n",
    "            count = 0\n",
    "            attr = torch.zeros(self.size_embed).to(self.device)\n",
    "            for token in str(x1[i]).split(\" \"):\n",
    "                token = token.replace(\".0\", \"\")\n",
    "                if token.lower() in self.words:\n",
    "                    attr = attr.add(torch.tensor(self.words[token.lower()]).to(self.device))\n",
    "                else:\n",
    "                    attr = attr.add(torch.tensor(self.words['<unk>']).to(self.device))\n",
    "                count += 1\n",
    "            t1.append(attr.div(count))\n",
    "        return torch.stack(t1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        t1 = self.create_embed(x1)\n",
    "        t2 = self.create_embed(x2)\n",
    "\n",
    "        sim = F.cosine_similarity(t1, t2, dim=1)\n",
    "        sim.requires_grad = True\n",
    "\n",
    "        out = self.fc1(sim.unsqueeze(0))\n",
    "        out = self.probs(self.fc2(out))\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
